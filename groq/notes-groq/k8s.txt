
# ======================================================
# =====[ WORKFLOW ]=====================================
# ======================================================
# get to right context and namespace
k8s-switch

# see current state of the cluster
~/git/dc-tools/tools/gv-dashboards.py -c

##### fix the missing / unhealthy nodes #####

# check validation status
~/git/dc-tools/tools/gv-dashboards.py -c

# find failed attempts only
kval-status --only-failed

# create ticket
THIS NEEDS TO BE LOOKED INTO USING KUBECTL REPAIR

# retry validation (TURN THIS INTO A FUNCTION)
    # retry from point of failure
    kubectl validation retry --node <rack-node>

    # removes existing validation from node, rack and xrk - It's like starting validation over
    kubectl validation requalify --node <rack-node>

# check source of truth
kracks <rack> [rack2] [...]

# ======================================================
# =====[ ADD NODE(S) TO CLUSTER ]==========================
# ======================================================
cd /Users/jjensen/git/infrastructure-platform-clusters/scripts
NODES=$(echo c1r{9..20}-gn{1..9}) ./bios-config-yaml-gen.sh && kubectl apply -f bios-config-jobs && rm -rf bios-config-jobs

# as nodes come up, run:
kubectl label nodes -l 'groq.node=true,!topology.groq.io/wiring' topology.groq.io/family=xt topology.groq.io/wiring=xt4444 topology.groq.io/power-pod-id=1 --overwrite


# =============================================
# =====[ BMC ]=================================
# =============================================
# checck power status
ipmitool -H <node> -U root -P GroqRocks1 power status


# =============================================
# =====[ PODS ]================================
# =============================================
# get pods for rack
kubectl get pods -n groq-system | grep $RACK

# restart tspd for entire rack
kubectl delete -n groq-system $(echo {1..9} | xargs -n1 | parallel -j9 kubectl get pod -n groq-system -l app=tspd --field-selector spec.nodeName=$RACK-gn{} --no-headers -o name)

# restart tspd for single rack
kubectl delete pod -n groq-system -l app=tspd --field-selector spec.nodeName=c0r67-gnX

# force delete stuck pod
kubectl -n groq-system delete pod --force $POD


# =============================================
# =====[ INFO ]================================
# =============================================
# current state of rack
kubectl get nodes -n groq-system| grep $RACK

# kubectl racks
kubectl racks | grep $RACK

# single rack metadata
kubectl racks -o json | jq '.items[] | select(.metadata.name=="c1r25")'


# =============================================
# =====[ MAINTENANCE ]=========================
# =============================================
# uncordon rack
for i in {1..9} ; do kubectl uncordon $RACK-gn$i ; done


# =============================================
# =====[ LOGS ]================================
# =============================================
# view logs
kubectl logs -n groq-system $POD | jq

# view conductor logs
kubectl logs -c conductor -n groq-system $POD

kubectl logs -c conductor $POD | egrep "$nova_ncp_reg" -o | sort -uV


# =============================================
# =====[ CONTEXT ]=============================
# =============================================
# add new context for tailscale
tailscale configure kubeconfig msp2-prod1.tail15648.ts.net

# rename context
kubectl config rename-context dmm1-prod1.tail15648.ts.net dmm1-prod1

# change context
kubectl config use-context dmm1-prod1.tail15648.ts.net

# single rack metadata
kubectl racks -o json | jq '.items[] | select(.metadata.name=="c1r25")'


# =============================================
# =====[ NODE ADMIN ]==========================
# =============================================
# images
fw updates - us-west1-docker.pkg.dev/prj-c-cloud-dcinfra-bc27/infrastructure-platform/tsp-ctl:55b7c210597
single chip and general - us-west1-docker.pkg.dev/prj-c-cloud-dcinfra-bc27/infrastructure-platform/tsp-ctl

# job templates
/infrastructure-platform-clusters/scripts/hw_validation/templates

# upgrade card fw
kubectl get node -l groq.fw-bundle-lowest=0.0.10
$HOME/git/infrastructure-platform-clusters/scripts/node-admin.sh <rack>-<node> us-west1-docker.pkg.dev/prj-c-cloud-dcinfra-bc27/infrastructure-platform/tsp-ctl:55b7c210597
kubectl taint node <node> firmware-update:NoExecute
kubectl taint node <node> firmware-update:NoSchedule
TSPCTL_NO_SYSTEMD=true tsp-ctl --device=/dev/<groq_card>.usb update-fw --bundle /firmware_bundle.bin *** OR *** TSPCTL_NO_SYSTEMD=true tsp-ctl update-fw --bundle /firmware_bundle.bin --all-devices
kubectl taint node <node> firmware-update:NoExecute-
kubectl taint node <node> firmware-update:NoSchedule-
# reboot node
# delete existing fw job

~/git/infrastructure-platform-clusters/scripts/fw-update-yaml-gen.sh 0.0.10
kubectl apply -f fw-update/<rack>


# find missing usb
tsp-ctl k8s-init


# misc fw update
while read rack ; do echo $rack ; kubectl describe nodes $rack | grep -i firmware ; done < fw-updates.txt
rm -rf $HOME/git/infrastructure-platform-clusters/scripts/fw-update-jobs/ ; while read rack ; do $HOME/git/infrastructure-platform-clusters/scripts/fw-update-yaml-gen.sh $rack 0.0.15 ; kubectl apply -f $HOME/git/infrastructure-platform-clusters/scripts/fw-update-jobs/firmware-update-$rack.yaml ; done < fw-updates.txt


# run single chip manually
$HOME/git/infrastructure-platform-clusters/scripts/node-admin.sh <rack>-<node> us-west1-docker.pkg.dev/prj-c-cloud-dcinfra-bc27/infrastructure-platform/tsp-ctl
/bin/single-chip-determinism-exe --iterations 1000 --return-failed-devices --verbose

# run single-node-link-test manually
$HOME/git/infrastructure-platform-clusters/scripts/node-admin.sh <rack>-<node> us-west1-docker.pkg.dev/prj-c-cloud-dcinfra-bc27/infrastructure-platform/tsp-ctl
single-node-link-test-exe --mode=All --return-failure --verbose

# functions
- label-check-rack
- knodes
- kracks
- k-tspd
- knodes-ipmi
- kracks-ipmi
- k-get-leases
# scripts
- clean_racks.sh
- gv_tui.py
- gv_cli.py
- kval_status.sh
- new_validation.sh
- xt_find_c2c_connection.py
- check_racks.sh
- fixbmcpw.sh
# aliases
- kval-logs
- kval-retry
- bin-groq
- tui
- conman
- kc
- kpods
- klist